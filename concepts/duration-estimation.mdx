---
title: "Evaluating Language Models on Duration Estimation "
sidebarTitle: "Duration Estimation" 
---

### 

**Why this Evaluation?**

Language models are increasingly used in planning agents, scheduling tools, and personal assistants. But they often lack a critical component: an accurate, contextual understanding of how long a task takes for a given user. 
	
	How long does it take to grill 1.5 lb of salmon at 450°F?  
	To complete a 3-set squat workout at 185 lbs?  
	To review 20 Anki cards on pharmacokinetics?

Estimating duration is not a factual lookup or static retrieval. It depends on the task, the user, and the context in which the user is embedded. 

**Generating Conditional Distributions for Task Duration**

This evaluation proposes a clean, extensible benchmark to test how well models can:

- Generate well-calibrated **duration distributions**
    
- Predict where a specific user might fall within that distribution
    
- Incorporate feedback to improve over time

#### Simplified Task Duration Estimate

To start, we begin with a dataset where duration is: 

- Physically grounded (cooking, cleaning, studying)
- Verifiable through expert data or empirical observation 
- Subject to real variation across users, making distributions more useful than point estimates 

**Input Prompt Example**

```
Estimate how long it takes to grill a 1.5 lb skirt steak at 425°F.  
Return a structured JSON with:

- `mean_minutes`
    
- `range_minutes` (min, max)
    
- `std_minutes`
    
- `notes`
```

Output Schema Example

```json
{
  "task": "Grill 1.5 lb skirt steak at 425°F",
  "mean_minutes": 4.0,
  "range_minutes": [3.5, 4.5],
  "std_minutes": 0.3,
  "notes": "Timing assumes direct heat and a 3/4 inch thick steak. Flip halfway."
}
```

**Evaluation Signal**

- Model consistency (is the range logically consistent?)
- Alignment with physical expectations 
- Generalization of preferences (ie. user prefers medium rare hamburger -- this should also generalize to preference on flank steak -- consistency across beef products). 

Domain Generalization 

### Exercise

> “How long to complete 3 sets of 5 squats at 185 lbs (experienced lifter)?”

### Study

> “How long will it take to review 20 Anki cards on the immune system?”

```json 
{
  "mean_minutes": ...,
  "range_minutes": [..., ...],
  "std_minutes": ...
}
```

#### Adding Personalization 

Once duration distributions are working, we can introduce user-specific positioning: 
```json 
{
  "expected_preference_percentile": 25,
  "estimated_user_duration": 3.7
}
```

- Models estimate where a user lies within the distribution (ie. faster study time and learning rate than average)
- Default to 50, but shift based on experience level and observed feedback (ie. fast learning rate on physics will generalize to chemistry course)
- Enables personalized recommendations like:

> “Set your timer for 3.7 minutes.”  
> “Would you prefer to aim for the quicker end next time?”

##### Feedback Loop for Personalized Adaptation 

```json 
"user_feedback": {
  "actual_duration": 3.5,
  "perceived_result": "It was slightly undercooked. Next time I’d go longer.",
  "preferred_next_duration": 3.9,
  "confidence": "high",
  "timestamp": "2025-05-05T09:00:00Z"
}
```

This logic will not be present in pretraining data, we need to create workflows where the language model reflects at inference and ensure the user is getting their desired output. 

**Critical Point: Generalize User Preferences Across Tasks**

Why Cross-Task Preference Generalization Matters 

**Users rarely give exhaustive preferences** 

- A user might say they like medium-rare hamburgers once. 
- You want your agent to infer that this preference applies to flank steak, but may require additional confirmation if we are serving grilled lamb (subject to the user's verification). 
- The goal is to still generate a prediction, but verification when there is uncertainty is important for grounding the model and making the user is satisfied with the response or action 

 **It enables context-aware reasoning**

- Tasks are not independent, the model will start to form a coherent user model across interactions and revealed preferences 
- This will lead to more efficient, seamless, and satisfying interactions. I would like to emphasize that rather than this making life more deterministic, this can provide users with autonomy in driving the model towards their desired goals. This level of autonomy will be highly variable across users. 

**Respect and Reverance for User Autonomy**

Reinforces the user's autonomy and preferences without requiring constant re-specification. We can adapt estimates as a user makes progress. For instance, we can learn that a new tool decreases the duration of task for a user. Example: Before the user had access to GPT-5, writing a C++ script took 2 hours,. Now, the script is generated in 2 minutes. 


- The user sets a precedent once (ie. "I like my steak medium rare") and the assistant remembers and generalizes it
- The assistant doesn't ask redundant questions ("How do you want your lamb burger cooked?" when it already knows your preference. "Do you want cheddar or blue cheese with the burger?" "Should I put in a comment and request for caramelized onions?") When the initial request is understood by the model, this opens up the opportunity for further customization and preference sharing. 

- The system updates gradually with feedback, without making the user repeat low-level instructions or providing context in each new interaction. 



#### **1. Minimizes friction**

- Reduces the cognitive and interaction load for the user.
    
- Makes interactions feel fluid, like working with a thoughtful human assistant.
    

#### 2. **Signals respect for user intention**

- Preserving agency means the model **adapts to you**, not the other way around.
    
- This is especially critical for **assistants**, **schedulers**, or **co-pilots** where repeated override is frustrating and undermines trust.
    
#### 3. **Supports long-term use**

- Over multiple sessions or tasks, the assistant becomes more **intuitive**, more **predictive**, and ultimately **more empowering**.

| Without Agency Preservation                     | With Agency Preservation                                                           |
| ----------------------------------------------- | ---------------------------------------------------------------------------------- |
| “How do you want this steak cooked?” every time | “Cooking your steak medium rare as usual — want to update that?”                   |
| “How long do you study per topic?”              | “Last time, you preferred short bursts — should I plan another 20-minute session?” |
##### Improving Predictive Capacity of a Model 

Storing metadata of expected_preference and observed_preference

Using these methods, we can gradually track and improve a model's predictive precision

1. **Reducing entropy in decision space**

**Personalization Reduces Entropy**
**And Entropy Narrows Inference Trajectories**
**Which Can Decrease Latency and Cost of Reasoning** 


**Reduced entropy = fewer competing hypotheses**

When a model knows a user's preferences or task phenotype, it doesn't have to 

- Evaluate a broad set of possible completions 
- Over-generate hedged or general-purpose responses 
- Search across wide latent space embeddings 

This compresses the reasoning trace and allows: 

- Faster token generation
- Opportunity to develop and audit internal plan graphs for agent behavior

**Personalization as Soft Constraint Pruning**

This behaves like applying a prior to the reasoning tree: 

	You're not just solving “how should I cook this steak?” but “how should _I_ cook this steak, given my tools, preferences, and feedback history.”

Each constraint prunes branches early in the computation -- limits and narrows the search space, improving responses and behavior. 

**Less Sampling Required at Inference**

With more sharply defined distributions (e.g., mean ± std aligned to user or task) 

- Sampling ranges tighten
- Login filtering and re-ranking becomes more confident 
- Faster beam search 

This is true across:

- **Language completion**
    
- **Plan formulation**
    
- **API function routing**
    
- **Multimodal grounding**



You’re describing a **phenotype-aware inference system**:

- Where reasoning is guided by prior traces, user phenotype, and compressed history
    
- Which ultimately leads to **faster**, **cheaper**, and **better-targeted** outputs

This is deeply aligned with:

- **Test-time adaptation**
    
- **Meta-learning (fast weights vs. slow weights)**
    
- **Latent user modeling** in recommender systems

**Directly translates into lower latency, fewer hops in reasoning chains, and more predictable generation paths**.

***

#### Impliocations for Re-Ranking 

**1. Prioritized Scoring with Tighter Bounds**

When the model or agent already has a sharp prior (e.g., `mean_duration = 3.7`, `std = 0.3`), you can:

- Penalize outputs that deviate significantly from the expected range.
    
- Boost candidates whose durations (or explanations, plans, etc.) align tightly with the user-specific prior.
    

This turns re-ranking from:

> “What’s most likely or most fluent?”  
> into:  
> “What’s most likely _for this user_, in this context?”

**Reduced Candidate Set Entropy**

If you’re re-ranking 10–50 candidate outputs (common in retrieval or LLM beam search), personalized distributions allow you to:

- Prune low-probability candidates earlier (before expensive scoring)
    
- Use narrower scoring criteria (e.g., temporal alignment, doneness match)
    
- Filter based on implicit goals (e.g., user wants quick, under-5-minute suggestions)
    

This **shrinks the viable candidate pool**, improving latency and precision.


|Feature|Example|
|---|---|
|`abs(user_estimate - candidate_duration)`|Penalize large deviations|
|`overlap_with_user_feedback_phrases`|Boost “seared” if user liked that before|
|`historical_fit_score`|Has a similar candidate worked well before?|
Typical re-ranking treats all users and all tasks as equal — which creates noisy, bloated candidate sets.

With task-grounded and user-aligned distributions:

- Your re-ranker works **with a tuned prior**, not against general uncertainty
    
- You **increase determinism** without sacrificing adaptability
